{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pdf2txt import PDFProcessor\n",
    "import pdfplumber\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print('1')\n",
    "except:\n",
    "    print('2')\n",
    "else:\n",
    "    file = './data/pdf/2021pdf年报/深圳能源：2021年年度报告.pdf'\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MDA(pdf_path):\n",
    "    '''\n",
    "    Extract the MDA parts from the pdf reports.\n",
    "    '''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        start_page = 0\n",
    "        end_page = 0\n",
    "        for i in range(10):  # 循环前10页，找到目录页\n",
    "            content = pdf.pages[i].extract_text()\n",
    "            if \".......\" in content:\n",
    "                content = content.split('\\n')\n",
    "                #print(content)\n",
    "                break\n",
    "        \n",
    "        for j in range(len(content)):\n",
    "            if \"与分析\" in content[j] or \"董事会\" in content[j]:\n",
    "                start_page = int(content[j][-2:])\n",
    "                end_page = int(content[j+1][-2:])\n",
    "                break\n",
    "        \n",
    "    return start_page, end_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/pdf/深圳能源：2021年年度报告.pdf'\n",
    "start_page, end_page = extract_MDA(file_path)\n",
    "processor = PDFProcessor(file_path)\n",
    "processor.process_pdf(start_page, end_page)\n",
    "processor.save_all_text(path='./data/txt/深圳能源：2021年年度报告.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_name = file_path.replace('pdf', 'txt')\n",
    "content_clear = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])\n",
    "        if json_str['type'] == 'text' and '。' in json_str['inside']: # json_str['inside'][-1] == '。'\n",
    "            sentences = json_str[\"inside\"].split('。')\n",
    "            if sentences[-1] == '':\n",
    "                sentences.pop()            \n",
    "            sentences = [item + '。\\n' for item in sentences]\n",
    "            content_clear.extend(sentences)\n",
    "\n",
    "save_path = file_path.replace('pdf', 'txt_clear')\n",
    "save_path = save_path[:-6] # delete the last \"_clear\" string\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(content_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 32,\n",
       " 'allrow': 708,\n",
       " 'type': 'text',\n",
       " 'inside': '5．深化规范运作，提升治理水平新高度。一方面坚持做好上市工作运作基础工作，持续加强董事会自身建设，认真做好信息披露和投资者关系维护，向市场有效传递公司转型升级、低碳环保和高质量发展形象；另一方面深入推广实施大合规'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "file_path = './data/pdf/深圳能源：2021年年度报告.pdf'\n",
    "text_name = file_path.replace('pdf','txt')\n",
    "content = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "json_str = json.loads(content[708])\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_name = file_path.replace('pdf', 'txt')\n",
    "str_clear = ''\n",
    "# content_clear = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])\n",
    "        if json_str['type'] == 'text' and '。' in json_str['inside']: # json_str['inside'][-1] == '。'\n",
    "            str_clear = str_clear + json_str[\"inside\"]\n",
    "sentences = str_clear.split('。')\n",
    "if sentences[-1] == '':\n",
    "    sentences.pop()\n",
    "content_clear = [item + '。\\n' for item in sentences]\n",
    "\n",
    "save_path = file_path.replace('pdf', 'txt_clear')\n",
    "save_path = save_path[:-6] # delete the last \"_clear\" string\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(content_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "txt_name = './data/txt/2021_002342_巨力索具.txt'\n",
    "with open(txt_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.readlines()\n",
    "    print(content)\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset_name = \"climatebert/climate_detection\"\n",
    "model_name = \"climatebert/distilroberta-base-climate-detector\"\n",
    "\n",
    "# If you want to use your own data, simply load them as 🤗 Datasets dataset, see https://huggingface.co/docs/datasets/loading\n",
    "dataset = datasets.load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_len=512)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline\n",
    "for out in tqdm(pipe(KeyDataset(dataset, \"text\"), padding=True, truncation=True)):\n",
    "   print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tokenizer_climatebert = AutoTokenizer.from_pretrained(\"./model/distilroberta-base-climate-detector\")\n",
    "model_climatebert = AutoModelForSequenceClassification.from_pretrained(\"./model/distilroberta-base-climate-detector\")\n",
    "\n",
    "tokenizer_zh2en = AutoTokenizer.from_pretrained(\"./model/opus-mt-zh-en\", use_fast=False)\n",
    "model_zh2en = AutoModelForSeq2SeqLM.from_pretrained(\"./model/opus-mt-zh-en\")\n",
    "\n",
    "pipe1 = pipeline(task='translation', model=model_zh2en, tokenizer=tokenizer_zh2en, device=0, truncation=True)\n",
    "pipe2 = pipeline(task='text-classification', model=model_climatebert, tokenizer=tokenizer_climatebert, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'yes', 'score': 0.998123824596405},\n",
       " {'label': 'no', 'score': 0.9112256169319153}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = '电化学能量存储装置是绿色、清洁能源的重要载体之一，在碳减排背景下广泛应用于绿色、清洁能源的存储、转换、使用，其重要性日益凸显。'\n",
    "text_2 = '通过套期保值操作可以规避商品价格波动、汇率波动对公司造成的影响，有利于公司的正常经营，但同时也可能存在一定风险。' \n",
    "\n",
    "text = [text_1, text_2]\n",
    "text\n",
    "\n",
    "en_text = [m['translation_text'] for m in pipe1(text)]\n",
    "pipe2(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Electrochemical energy storage is one of the important carriers of green and clean energy, which is increasingly important in the context of carbon emission reductions and is widely applied to the storage, conversion and use of green and clean energy.'},\n",
       " {'translation_text': 'The impact on companies of commodity price fluctuations and exchange rate fluctuations can be avoided through hedging operations, which facilitates the normal operation of companies, but at the same time there may be some risk.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sentences, yes_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "txt_path = './data/txt_clear/2021'\n",
    "txt_list = os.listdir(txt_path)\n",
    "# txt_path + '/' + txt_list[0]\n",
    "#with open(txt_path+'/'+txt_list[0], 'r', encoding='utf-8') as f:\n",
    "with open('./data/txt_clear/2019_002586_ST围海.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    total_sentences = len(content)\n",
    "    for i in range(total_sentences):\n",
    "        content[i] = content[i].replace('\\n', '')\n",
    "    en_text = [m['translation_text'] for m in pipe1(content)]\n",
    "    result = pipe2(en_text)\n",
    "    yes_elements = [item for item in result if item['label'] == 'yes']\n",
    "    yes_sentences = len(yes_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# model_path = 'C:/Users/Administrator/.cache/huggingface/hub'\n",
    "model_path = './model'\n",
    "\n",
    "tokenizer_climatebert = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-detector\")\n",
    "model_climatebert = AutoModelForSequenceClassification.from_pretrained(\"climatebert/distilroberta-base-climate-detector\")\n",
    "\n",
    "tokenizer_zh2en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", use_fast=False)\n",
    "model_zh2en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "def model_compute(sentence):\n",
    "    # Translate Chinese sentence into English\n",
    "    batch = tokenizer_zh2en([sentence], return_tensors=\"pt\")\n",
    "    generated_ids = model_zh2en.generate(**batch)\n",
    "    tokenizer_zh2en.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Detect whether the sentence is related to climate risk\n",
    "    inputs = tokenizer_climatebert(tokenizer_zh2en.batch_decode(generated_ids, skip_special_tokens=True)[0], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model_climatebert(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return model_climatebert.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# 创建一个日志记录器\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 设置日志记录器的级别\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 创建一个文件处理器，用于将日志信息写入文件\n",
    "file_handler = logging.FileHandler('output.log', mode='a')  # 'a' 表示追加模式\n",
    "\n",
    "# 设置文件处理器的日志级别为ERROR，这样只有ERROR和CRITICAL级别的日志会被写入文件\n",
    "file_handler.setLevel(logging.ERROR)\n",
    "\n",
    "# 创建一个格式化器，用于定义日志的输出格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 设置文件处理器的格式\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# 将文件处理器添加到日志记录器\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(10):\n",
    "        logger.info(f\"{i}已保存.\")\n",
    "        logger.error(f\"{i}报错.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='example.log', filemode='w', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(10):\n",
    "        logging.info(f\"{i}已保存.\")\n",
    "        logging.error(f\"{i}报错.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_convert debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pdfplumber\n",
    "import logging\n",
    "from pdf2txt import PDFProcessor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "flag_pdf = True\n",
    "pdf_file_path = './data/pdf/2021pdf年报\\\\2021_000638_万方发展.pdf'\n",
    "txt_file_path = './data/txt/2021txt年报\\\\2021_000638_万方发展.txt'\n",
    "pdf_url = 'http://static.cninfo.com.cn/finalpage/2022-10-20/1214843056.PDF'\n",
    "year = 2021\n",
    "code = 638\n",
    "name = '万方发展'\n",
    "\n",
    "def download_pdf(pdf_url, pdf_file_path):\n",
    "    try:\n",
    "        with requests.get(pdf_url, stream=True, timeout=10) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(pdf_file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"下载PDF文件失败：{e}\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 下载PDF文件\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        retry_count = 3\n",
    "        while retry_count > 0:\n",
    "            if download_pdf(pdf_url, pdf_file_path):\n",
    "                break\n",
    "            else:\n",
    "                retry_count -= 1\n",
    "        if retry_count == 0:\n",
    "            logging.error(f\"下载失败：{pdf_url}\")\n",
    "            #return\n",
    "        \n",
    "    # 提取MDA部分页码，转换PDF文件为TXT文件\n",
    "    start_page, end_page = extract_MDA(pdf_file_path)\n",
    "    processor = PDFProcessor(pdf_file_path)\n",
    "    processor.process_pdf(start_page, end_page)\n",
    "    processor.save_all_text(path=txt_file_path)\n",
    "    processor.pdf.close() # 此处必须手动关闭pdf，因为PDFProcessor类定义中没有使用with\n",
    "\n",
    "    logging.info(f\"{txt_file_path} 已保存.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"处理 {year}_{code:06}_{name}时出错： {e}\")\n",
    "else:\n",
    "    # 删除已转换的PDF文件，以节省空间\n",
    "    if flag_pdf:\n",
    "        os.remove(pdf_file_path)\n",
    "        logging.info(f\"{pdf_file_path} 已被删除.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '分析'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1212711064.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mextract_MDA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mextract_MDA\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content)):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m与分析\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m董事会\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j]:\n\u001b[1;32m---> 17\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         end_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(content[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '分析'"
     ]
    }
   ],
   "source": [
    "pdf_path = '1212711064.pdf'\n",
    "extract_MDA(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '分析'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m与分析\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m董事会\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j]:\n\u001b[1;32m---> 13\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         end_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(content[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '分析'"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    start_page = 0\n",
    "    end_page = 0\n",
    "    for i in range(10):  # 循环前10页，找到目录页\n",
    "        content = pdf.pages[i].extract_text()\n",
    "        if \".......\" in content:\n",
    "            content = content.split('\\n')\n",
    "            #print(content)\n",
    "            break\n",
    "    \n",
    "    for j in range(len(content)):\n",
    "        if \"与分析\" in content[j] or \"董事会\" in content[j]:\n",
    "            start_page = int(content[j][-2:])\n",
    "            end_page = int(content[j+1][-2:])\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    start_page = 0\n",
    "    end_page = 0\n",
    "    content = pdf.pages[i].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "深圳市中金岭南有色金属股份有限公司2021年年度报告全文\n",
      "目录\n",
      "第一节 重要提示、目录和释义\n",
      ".........................................................................................................................................2\n",
      "第二节 公司简介和主要财务指标\n",
      "....................................................................................................................................6\n",
      "第三节 管理层讨论与分析\n",
      ".................................................................................................................................................. 11\n",
      "第四节 公司治理\n",
      "....................................................................................................................................................................... 47\n",
      "第五节 环境和社会责任\n",
      "....................................................................................................................................................... 67\n",
      "第六节 重要事项\n",
      "....................................................................................................................................................................... 72\n",
      "第七节 股份变动及股东情况\n",
      "............................................................................................................................................. 90\n",
      "第八节 优先股相关情况\n",
      "....................................................................................................................................................... 98\n",
      "第九节 债券相关情况\n",
      "............................................................................................................................................................ 99\n",
      "第十节 财务报告\n",
      "..................................................................................................................................................................... 102\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import os\n",
    "\n",
    "year = 2021\n",
    "txt_path = f'./data/pdf/{year}pdf年报'\n",
    "txt_list = os.listdir(txt_path)\n",
    "workbook = openpyxl.load_workbook(f'./data/excel/{year}年报remain.xlsx')\n",
    "worksheet = workbook['Sheet1']\n",
    "\n",
    "for txt_name in txt_list:\n",
    "    year, code, name = txt_name.replace('.pdf', '').split('_')\n",
    "    code = int(code)\n",
    "    worksheet.append([code, name])\n",
    "\n",
    "workbook.save(f'./data/excel/{year}年报remain.xlsx')\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
