{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pdf2txt import PDFProcessor\n",
    "import pdfplumber\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    print('1')\n",
    "except:\n",
    "    print('2')\n",
    "else:\n",
    "    file = './data/pdf/2021pdfå¹´æŠ¥/æ·±åœ³èƒ½æºï¼š2021å¹´å¹´åº¦æŠ¥å‘Š.pdf'\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_MDA(pdf_path):\n",
    "    '''\n",
    "    Extract the MDA parts from the pdf reports.\n",
    "    '''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        start_page = 0\n",
    "        end_page = 0\n",
    "        for i in range(10):  # å¾ªç¯å‰10é¡µï¼Œæ‰¾åˆ°ç›®å½•é¡µ\n",
    "            content = pdf.pages[i].extract_text()\n",
    "            if \".......\" in content:\n",
    "                content = content.split('\\n')\n",
    "                #print(content)\n",
    "                break\n",
    "        \n",
    "        for j in range(len(content)):\n",
    "            if \"ä¸åˆ†æ\" in content[j] or \"è‘£äº‹ä¼š\" in content[j]:\n",
    "                start_page = int(content[j][-2:])\n",
    "                end_page = int(content[j+1][-2:])\n",
    "                break\n",
    "        \n",
    "    return start_page, end_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/pdf/æ·±åœ³èƒ½æºï¼š2021å¹´å¹´åº¦æŠ¥å‘Š.pdf'\n",
    "start_page, end_page = extract_MDA(file_path)\n",
    "processor = PDFProcessor(file_path)\n",
    "processor.process_pdf(start_page, end_page)\n",
    "processor.save_all_text(path='./data/txt/æ·±åœ³èƒ½æºï¼š2021å¹´å¹´åº¦æŠ¥å‘Š.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clear txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_name = file_path.replace('pdf', 'txt')\n",
    "content_clear = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])\n",
    "        if json_str['type'] == 'text' and 'ã€‚' in json_str['inside']: # json_str['inside'][-1] == 'ã€‚'\n",
    "            sentences = json_str[\"inside\"].split('ã€‚')\n",
    "            if sentences[-1] == '':\n",
    "                sentences.pop()            \n",
    "            sentences = [item + 'ã€‚\\n' for item in sentences]\n",
    "            content_clear.extend(sentences)\n",
    "\n",
    "save_path = file_path.replace('pdf', 'txt_clear')\n",
    "save_path = save_path[:-6] # delete the last \"_clear\" string\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(content_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 32,\n",
       " 'allrow': 708,\n",
       " 'type': 'text',\n",
       " 'inside': '5ï¼æ·±åŒ–è§„èŒƒè¿ä½œï¼Œæå‡æ²»ç†æ°´å¹³æ–°é«˜åº¦ã€‚ä¸€æ–¹é¢åšæŒåšå¥½ä¸Šå¸‚å·¥ä½œè¿ä½œåŸºç¡€å·¥ä½œï¼ŒæŒç»­åŠ å¼ºè‘£äº‹ä¼šè‡ªèº«å»ºè®¾ï¼Œè®¤çœŸåšå¥½ä¿¡æ¯æŠ«éœ²å’ŒæŠ•èµ„è€…å…³ç³»ç»´æŠ¤ï¼Œå‘å¸‚åœºæœ‰æ•ˆä¼ é€’å…¬å¸è½¬å‹å‡çº§ã€ä½ç¢³ç¯ä¿å’Œé«˜è´¨é‡å‘å±•å½¢è±¡ï¼›å¦ä¸€æ–¹é¢æ·±å…¥æ¨å¹¿å®æ–½å¤§åˆè§„'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "file_path = './data/pdf/æ·±åœ³èƒ½æºï¼š2021å¹´å¹´åº¦æŠ¥å‘Š.pdf'\n",
    "text_name = file_path.replace('pdf','txt')\n",
    "content = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "json_str = json.loads(content[708])\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_name = file_path.replace('pdf', 'txt')\n",
    "str_clear = ''\n",
    "# content_clear = []\n",
    "with open(text_name, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])\n",
    "        if json_str['type'] == 'text' and 'ã€‚' in json_str['inside']: # json_str['inside'][-1] == 'ã€‚'\n",
    "            str_clear = str_clear + json_str[\"inside\"]\n",
    "sentences = str_clear.split('ã€‚')\n",
    "if sentences[-1] == '':\n",
    "    sentences.pop()\n",
    "content_clear = [item + 'ã€‚\\n' for item in sentences]\n",
    "\n",
    "save_path = file_path.replace('pdf', 'txt_clear')\n",
    "save_path = save_path[:-6] # delete the last \"_clear\" string\n",
    "with open(save_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(content_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "txt_name = './data/txt/2021_002342_å·¨åŠ›ç´¢å…·.txt'\n",
    "with open(txt_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.readlines()\n",
    "    print(content)\n",
    "    for i in range(len(content)):\n",
    "        json_str = json.loads(content[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### caculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dataset_name = \"climatebert/climate_detection\"\n",
    "model_name = \"climatebert/distilroberta-base-climate-detector\"\n",
    "\n",
    "# If you want to use your own data, simply load them as ğŸ¤— Datasets dataset, see https://huggingface.co/docs/datasets/loading\n",
    "dataset = datasets.load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_len=512)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# See https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline\n",
    "for out in tqdm(pipe(KeyDataset(dataset, \"text\"), padding=True, truncation=True)):\n",
    "   print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Python\\Anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tokenizer_climatebert = AutoTokenizer.from_pretrained(\"./model/distilroberta-base-climate-detector\")\n",
    "model_climatebert = AutoModelForSequenceClassification.from_pretrained(\"./model/distilroberta-base-climate-detector\")\n",
    "\n",
    "tokenizer_zh2en = AutoTokenizer.from_pretrained(\"./model/opus-mt-zh-en\", use_fast=False)\n",
    "model_zh2en = AutoModelForSeq2SeqLM.from_pretrained(\"./model/opus-mt-zh-en\")\n",
    "\n",
    "pipe1 = pipeline(task='translation', model=model_zh2en, tokenizer=tokenizer_zh2en, device=0, truncation=True)\n",
    "pipe2 = pipeline(task='text-classification', model=model_climatebert, tokenizer=tokenizer_climatebert, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'yes', 'score': 0.998123824596405},\n",
       " {'label': 'no', 'score': 0.9112256169319153}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = 'ç”µåŒ–å­¦èƒ½é‡å­˜å‚¨è£…ç½®æ˜¯ç»¿è‰²ã€æ¸…æ´èƒ½æºçš„é‡è¦è½½ä½“ä¹‹ä¸€ï¼Œåœ¨ç¢³å‡æ’èƒŒæ™¯ä¸‹å¹¿æ³›åº”ç”¨äºç»¿è‰²ã€æ¸…æ´èƒ½æºçš„å­˜å‚¨ã€è½¬æ¢ã€ä½¿ç”¨ï¼Œå…¶é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚'\n",
    "text_2 = 'é€šè¿‡å¥—æœŸä¿å€¼æ“ä½œå¯ä»¥è§„é¿å•†å“ä»·æ ¼æ³¢åŠ¨ã€æ±‡ç‡æ³¢åŠ¨å¯¹å…¬å¸é€ æˆçš„å½±å“ï¼Œæœ‰åˆ©äºå…¬å¸çš„æ­£å¸¸ç»è¥ï¼Œä½†åŒæ—¶ä¹Ÿå¯èƒ½å­˜åœ¨ä¸€å®šé£é™©ã€‚' \n",
    "\n",
    "text = [text_1, text_2]\n",
    "text\n",
    "\n",
    "en_text = [m['translation_text'] for m in pipe1(text)]\n",
    "pipe2(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Electrochemical energy storage is one of the important carriers of green and clean energy, which is increasingly important in the context of carbon emission reductions and is widely applied to the storage, conversion and use of green and clean energy.'},\n",
       " {'translation_text': 'The impact on companies of commodity price fluctuations and exchange rate fluctuations can be avoided through hedging operations, which facilitates the normal operation of companies, but at the same time there may be some risk.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sentences, yes_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "txt_path = './data/txt_clear/2021'\n",
    "txt_list = os.listdir(txt_path)\n",
    "# txt_path + '/' + txt_list[0]\n",
    "#with open(txt_path+'/'+txt_list[0], 'r', encoding='utf-8') as f:\n",
    "with open('./data/txt_clear/2019_002586_STå›´æµ·.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    total_sentences = len(content)\n",
    "    for i in range(total_sentences):\n",
    "        content[i] = content[i].replace('\\n', '')\n",
    "    en_text = [m['translation_text'] for m in pipe1(content)]\n",
    "    result = pipe2(en_text)\n",
    "    yes_elements = [item for item in result if item['label'] == 'yes']\n",
    "    yes_sentences = len(yes_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# model_path = 'C:/Users/Administrator/.cache/huggingface/hub'\n",
    "model_path = './model'\n",
    "\n",
    "tokenizer_climatebert = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-detector\")\n",
    "model_climatebert = AutoModelForSequenceClassification.from_pretrained(\"climatebert/distilroberta-base-climate-detector\")\n",
    "\n",
    "tokenizer_zh2en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", use_fast=False)\n",
    "model_zh2en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "def model_compute(sentence):\n",
    "    # Translate Chinese sentence into English\n",
    "    batch = tokenizer_zh2en([sentence], return_tensors=\"pt\")\n",
    "    generated_ids = model_zh2en.generate(**batch)\n",
    "    tokenizer_zh2en.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Detect whether the sentence is related to climate risk\n",
    "    inputs = tokenizer_climatebert(tokenizer_zh2en.batch_decode(generated_ids, skip_special_tokens=True)[0], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model_climatebert(**inputs).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    return model_climatebert.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ—¥å¿—è®°å½•å™¨\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# è®¾ç½®æ—¥å¿—è®°å½•å™¨çš„çº§åˆ«\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºå°†æ—¥å¿—ä¿¡æ¯å†™å…¥æ–‡ä»¶\n",
    "file_handler = logging.FileHandler('output.log', mode='a')  # 'a' è¡¨ç¤ºè¿½åŠ æ¨¡å¼\n",
    "\n",
    "# è®¾ç½®æ–‡ä»¶å¤„ç†å™¨çš„æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œè¿™æ ·åªæœ‰ERRORå’ŒCRITICALçº§åˆ«çš„æ—¥å¿—ä¼šè¢«å†™å…¥æ–‡ä»¶\n",
    "file_handler.setLevel(logging.ERROR)\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ ¼å¼åŒ–å™¨ï¼Œç”¨äºå®šä¹‰æ—¥å¿—çš„è¾“å‡ºæ ¼å¼\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# è®¾ç½®æ–‡ä»¶å¤„ç†å™¨çš„æ ¼å¼\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# å°†æ–‡ä»¶å¤„ç†å™¨æ·»åŠ åˆ°æ—¥å¿—è®°å½•å™¨\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(10):\n",
    "        logger.info(f\"{i}å·²ä¿å­˜.\")\n",
    "        logger.error(f\"{i}æŠ¥é”™.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='example.log', filemode='w', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(10):\n",
    "        logging.info(f\"{i}å·²ä¿å­˜.\")\n",
    "        logging.error(f\"{i}æŠ¥é”™.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_convert debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pdfplumber\n",
    "import logging\n",
    "from pdf2txt import PDFProcessor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "flag_pdf = True\n",
    "pdf_file_path = './data/pdf/2021pdfå¹´æŠ¥\\\\2021_000638_ä¸‡æ–¹å‘å±•.pdf'\n",
    "txt_file_path = './data/txt/2021txtå¹´æŠ¥\\\\2021_000638_ä¸‡æ–¹å‘å±•.txt'\n",
    "pdf_url = 'http://static.cninfo.com.cn/finalpage/2022-10-20/1214843056.PDF'\n",
    "year = 2021\n",
    "code = 638\n",
    "name = 'ä¸‡æ–¹å‘å±•'\n",
    "\n",
    "def download_pdf(pdf_url, pdf_file_path):\n",
    "    try:\n",
    "        with requests.get(pdf_url, stream=True, timeout=10) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(pdf_file_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"ä¸‹è½½PDFæ–‡ä»¶å¤±è´¥ï¼š{e}\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # ä¸‹è½½PDFæ–‡ä»¶\n",
    "    if not os.path.exists(pdf_file_path):\n",
    "        retry_count = 3\n",
    "        while retry_count > 0:\n",
    "            if download_pdf(pdf_url, pdf_file_path):\n",
    "                break\n",
    "            else:\n",
    "                retry_count -= 1\n",
    "        if retry_count == 0:\n",
    "            logging.error(f\"ä¸‹è½½å¤±è´¥ï¼š{pdf_url}\")\n",
    "            #return\n",
    "        \n",
    "    # æå–MDAéƒ¨åˆ†é¡µç ï¼Œè½¬æ¢PDFæ–‡ä»¶ä¸ºTXTæ–‡ä»¶\n",
    "    start_page, end_page = extract_MDA(pdf_file_path)\n",
    "    processor = PDFProcessor(pdf_file_path)\n",
    "    processor.process_pdf(start_page, end_page)\n",
    "    processor.save_all_text(path=txt_file_path)\n",
    "    processor.pdf.close() # æ­¤å¤„å¿…é¡»æ‰‹åŠ¨å…³é—­pdfï¼Œå› ä¸ºPDFProcessorç±»å®šä¹‰ä¸­æ²¡æœ‰ä½¿ç”¨with\n",
    "\n",
    "    logging.info(f\"{txt_file_path} å·²ä¿å­˜.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"å¤„ç† {year}_{code:06}_{name}æ—¶å‡ºé”™ï¼š {e}\")\n",
    "else:\n",
    "    # åˆ é™¤å·²è½¬æ¢çš„PDFæ–‡ä»¶ï¼Œä»¥èŠ‚çœç©ºé—´\n",
    "    if flag_pdf:\n",
    "        os.remove(pdf_file_path)\n",
    "        logging.info(f\"{pdf_file_path} å·²è¢«åˆ é™¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'åˆ†æ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1212711064.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mextract_MDA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mextract_MDA\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content)):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä¸åˆ†æ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè‘£äº‹ä¼š\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j]:\n\u001b[1;32m---> 17\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         end_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(content[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'åˆ†æ'"
     ]
    }
   ],
   "source": [
    "pdf_path = '1212711064.pdf'\n",
    "extract_MDA(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'åˆ†æ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(content)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä¸åˆ†æ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mè‘£äº‹ä¼š\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m content[j]:\n\u001b[1;32m---> 13\u001b[0m         start_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         end_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(content[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'åˆ†æ'"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    start_page = 0\n",
    "    end_page = 0\n",
    "    for i in range(10):  # å¾ªç¯å‰10é¡µï¼Œæ‰¾åˆ°ç›®å½•é¡µ\n",
    "        content = pdf.pages[i].extract_text()\n",
    "        if \".......\" in content:\n",
    "            content = content.split('\\n')\n",
    "            #print(content)\n",
    "            break\n",
    "    \n",
    "    for j in range(len(content)):\n",
    "        if \"ä¸åˆ†æ\" in content[j] or \"è‘£äº‹ä¼š\" in content[j]:\n",
    "            start_page = int(content[j][-2:])\n",
    "            end_page = int(content[j+1][-2:])\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    start_page = 0\n",
    "    end_page = 0\n",
    "    content = pdf.pages[i].extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ·±åœ³å¸‚ä¸­é‡‘å²­å—æœ‰è‰²é‡‘å±è‚¡ä»½æœ‰é™å…¬å¸2021å¹´å¹´åº¦æŠ¥å‘Šå…¨æ–‡\n",
      "ç›®å½•\n",
      "ç¬¬ä¸€èŠ‚ é‡è¦æç¤ºã€ç›®å½•å’Œé‡Šä¹‰\n",
      ".........................................................................................................................................2\n",
      "ç¬¬äºŒèŠ‚ å…¬å¸ç®€ä»‹å’Œä¸»è¦è´¢åŠ¡æŒ‡æ ‡\n",
      "....................................................................................................................................6\n",
      "ç¬¬ä¸‰èŠ‚ ç®¡ç†å±‚è®¨è®ºä¸åˆ†æ\n",
      ".................................................................................................................................................. 11\n",
      "ç¬¬å››èŠ‚ å…¬å¸æ²»ç†\n",
      "....................................................................................................................................................................... 47\n",
      "ç¬¬äº”èŠ‚ ç¯å¢ƒå’Œç¤¾ä¼šè´£ä»»\n",
      "....................................................................................................................................................... 67\n",
      "ç¬¬å…­èŠ‚ é‡è¦äº‹é¡¹\n",
      "....................................................................................................................................................................... 72\n",
      "ç¬¬ä¸ƒèŠ‚ è‚¡ä»½å˜åŠ¨åŠè‚¡ä¸œæƒ…å†µ\n",
      "............................................................................................................................................. 90\n",
      "ç¬¬å…«èŠ‚ ä¼˜å…ˆè‚¡ç›¸å…³æƒ…å†µ\n",
      "....................................................................................................................................................... 98\n",
      "ç¬¬ä¹èŠ‚ å€ºåˆ¸ç›¸å…³æƒ…å†µ\n",
      "............................................................................................................................................................ 99\n",
      "ç¬¬åèŠ‚ è´¢åŠ¡æŠ¥å‘Š\n",
      "..................................................................................................................................................................... 102\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import os\n",
    "\n",
    "year = 2021\n",
    "txt_path = f'./data/pdf/{year}pdfå¹´æŠ¥'\n",
    "txt_list = os.listdir(txt_path)\n",
    "workbook = openpyxl.load_workbook(f'./data/excel/{year}å¹´æŠ¥remain.xlsx')\n",
    "worksheet = workbook['Sheet1']\n",
    "\n",
    "for txt_name in txt_list:\n",
    "    year, code, name = txt_name.replace('.pdf', '').split('_')\n",
    "    code = int(code)\n",
    "    worksheet.append([code, name])\n",
    "\n",
    "workbook.save(f'./data/excel/{year}å¹´æŠ¥remain.xlsx')\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
