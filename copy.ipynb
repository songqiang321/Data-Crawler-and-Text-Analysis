{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from opencc import OpenCC\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "!ls ../package/\n",
    "sys.path.insert(0, \"../package/\")\n",
    "from ltp import LTP\n",
    "nlp = LTP(path=\"base\")\n",
    "\n",
    "class TextCleaner:\n",
    "    '''\n",
    "        批量清洗数据\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 remove_space=True, # 去除空格\n",
    "                 remove_suspension=True, # 转换省略号\n",
    "                 only_zh=False, # 只保留汉子\n",
    "                 remove_sentiment_character=True, # 去除表情符号\n",
    "                 to_simple=True, # 转化为简体中文\n",
    "                 remove_html_label=True,\n",
    "                 remove_stop_words=False,\n",
    "                 stop_words_dir=\"./停用词/\",\n",
    "                 with_space=False,\n",
    "                 batch_size=256):\n",
    "        self._remove_space = remove_space\n",
    "        self._remove_suspension = remove_suspension\n",
    "        self._remove_sentiment_character = remove_sentiment_character\n",
    "\n",
    "        self._only_zh = only_zh\n",
    "        self._to_simple = to_simple\n",
    "\n",
    "        self._remove_html_label = remove_html_label\n",
    "        self._remove_stop_words = remove_stop_words\n",
    "        self._stop_words_dir = stop_words_dir\n",
    "\n",
    "        self._with_space = with_space\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def clean_single_text(self, text):\n",
    "        if self._remove_space:\n",
    "            text = self.remove_space(text)\n",
    "        if self._remove_suspension:\n",
    "            text = self.remove_suspension(text)\n",
    "        if self._remove_sentiment_character:\n",
    "            text = self.remove_sentiment_character(text)\n",
    "        if self._to_simple:\n",
    "            text = self.to_simple(text)\n",
    "        if self._only_zh:\n",
    "            text = self.get_zh_only(text)\n",
    "        if self._remove_html_label:\n",
    "            text = self.remove_html(text)\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text_list):\n",
    "        text_list = [self.clean_single_text(text) for text in tqdm(text_list)]\n",
    "        tokenized_words_list = self.tokenizer_batch_text(text_list)\n",
    "        if self._remove_stop_words:\n",
    "            text_list = [self.remove_stop_words(words_list, self._stop_words_dir, self._with_space) for words_list in tokenized_words_list]\n",
    "        return text_list\n",
    "\n",
    "    def remove_space(self, text):     #定义函数\n",
    "        return text.replace(' ','')   # 去掉文本中的空格\n",
    "\n",
    "    def remove_suspension(self, text):\n",
    "        return text.replace('...', '。')\n",
    "\n",
    "    def get_zh_only(self, text):\n",
    "        def is_chinese(uchar):\n",
    "            if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':  # 判断一个uchar是否是汉字\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        content = ''\n",
    "        for i in text:\n",
    "            if is_chinese(i):\n",
    "                content = content+i\n",
    "        return content\n",
    "\n",
    "    def remove_sentiment_character(self, sentence):    \n",
    "        pattern = re.compile(\"[^\\u4e00-\\u9fa5^,^.^!^，^。^?^？^！^a-z^A-Z^0-9]\")  #只保留中英文、数字和符号，去掉其他东西\n",
    "        #若只保留中英文和数字，则替换为[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\n",
    "        line = re.sub(pattern,'',sentence)  #把文本中匹配到的字符替换成空字符\n",
    "        new_sentence=''.join(line.split())    #去除空白\n",
    "        return new_sentence\n",
    "\n",
    "    def to_simple(self, sentence):\n",
    "        new_sentence = OpenCC('t2s').convert(sentence)   # 繁体转为简体\n",
    "        return new_sentence\n",
    "\n",
    "    def to_tradition(self, sentence):\n",
    "        new_sentence = OpenCC('s2t').convert(sentence)   # 简体转为繁体\n",
    "        return new_sentence\n",
    "\n",
    "    def remove_html(self, text):\n",
    "        return BeautifulSoup(text, 'html.parser').get_text() #去掉html标签\n",
    "\n",
    "    def tokenizer_batch_text(self, text_list):\n",
    "        tokenized_text = []\n",
    "        len_text = len(text_list)\n",
    "        with torch.no_grad():\n",
    "            steps = self._batch_size\n",
    "            for start_idx in tqdm(range(0, len_text, steps)):\n",
    "                if start_idx + steps > len_text:\n",
    "                    tokenized_text += nlp.seg(text_list[start_idx:])[0]\n",
    "                else:\n",
    "                    tokenized_text += nlp.seg(text_list[start_idx:start_idx+steps])[0]\n",
    "        return tokenized_text\n",
    "\n",
    "    def remove_stop_words(self, words_list, stop_words_dir, with_space=False):\n",
    "        \"\"\"\n",
    "        中文数据清洗  stopwords_chineses.txt存放在博客园文件中\n",
    "        :param text:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        stop_word_filepath_list = glob(stop_words_dir + \"/*.txt\")\n",
    "        for stop_word_filepath in stop_word_filepath_list:\n",
    "            with open(stop_word_filepath) as fp:\n",
    "                stopwords = {}.fromkeys([line.rstrip() for line in fp]) #加载停用词(中文)\n",
    "        eng_stopwords = set(stopwords) #去掉重复的词\n",
    "        words = [w for w in words_list if w not in eng_stopwords] #去除文本中的停用词\n",
    "        if with_space:\n",
    "            return ' '.join(words)\n",
    "        else:\n",
    "            return ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pretreatment(): \n",
    "    #加载数据\n",
    "    data_df=pd.read_csv('sohu.txt',sep='\\t',header=None)\n",
    "    data_df.columns=['分类','文章']\n",
    "    #读取停顿词列表\n",
    "    stopword_list=[k.strip() for k in open('stopwords.txt',encoding='utf8').readlines() if k.strip()!='']\n",
    "    #对样本循环遍历，使用jieba库的cut方法获得分词列表，判断此分词是否为停用词，如果不是停用词赋值给变量cutWords\n",
    "    cutWords_list=[]\n",
    "    for article in data_df['文章']:\n",
    "        cutWords=[k for k in jieba.cut(article) if k not in stopword_list]\n",
    "        cutWords_list.append(cutWords)\n",
    "    #由于分词过程较为耗时，将分词结果保存为本地文件cutWords_list.txt，其后就可以直接读取本地文件\n",
    "    with open('cutWords_list.txt','w') as file:\n",
    "        for cutWordsin in cutWords_list:\n",
    "            file.write(' '.join(cutWords)+'\\n')  \n",
    "    #读取已保存的分词文件\n",
    "    with open('cutWords_list.txt') as file:\n",
    "        cutWords_list=[k.split() for k in file.readlines()]\n",
    "\n",
    "    #文本特征提取及向量化\n",
    "    tfidf=TfidfVectorizer(cutWords_list,stop_words=stopword_list,min_df=40,max_df=0.3)\n",
    "    X=tfidf.fit_transform(data_df['文章'])\n",
    "    labelEncoder=LabelEncoder()\n",
    "    y=labelEncoder.fit_transform(data_df['分类'])\n",
    "    with open('tfidf_feature.pkl','wb') as file:  #将tfidf特征写入文件，之后可以直接读取该文件\n",
    "        save={'featureMatrix':X,'label':y}\n",
    "        pickle.dump(save,file)\n",
    "    \n",
    "    print('文本分类数据预处理完成！')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
